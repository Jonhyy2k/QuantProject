import os
import json
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, GRU, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import websocket
import time
from threading import Thread
from collections import deque
import random
import traceback
import sys
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# Load environment variables from .env file
load_dotenv()

# XTB API credentials (from environment variables)
XTB_USER_ID = os.environ.get("XTB_USER_ID", "50540163")  # Fallback to provided ID if env var not set
XTB_PASSWORD = os.environ.get("XTB_PASSWORD", "Jphost2005")  # Fallback to provided password if env var not set
XTB_WS_URL = os.environ.get("XTB_WS_URL", "wss://ws.xtb.com/real")  # Demo server; use "real" for live accounts

# Output file
OUTPUT_FILE = "XTB_STOCK_DATA_SET.txt"

# Global settings for batch processing - maximized for M1 iMac
MAX_STOCKS_PER_BATCH = 300  # Increased for powerful CPU
BATCH_DELAY = 10  # Reduced delay between batches since processing is fast
MAX_EXECUTION_TIME_PER_STOCK = 600  # 10 minutes max per stock for extremely thorough analysis
MAX_TOTAL_RUNTIME = 240 * 3600  # 240 hours (10 days) maximum total runtime


# WebSocket connection manager with improved reconnection and heartbeat
class XTBClient:
    def __init__(self):
        self.ws = None
        self.logged_in = False
        self.response_data = {}
        self.last_command = None
        self.reconnect_count = 0
        self.max_reconnects = 8  # Increased from 5
        self.running = True
        self.heartbeat_thread = None
        self.command_lock = False  # Simple lock for commands

    def on_open(self, ws):
        print("[INFO] WebSocket connection opened.")
        self.reconnect_count = 0  # Reset reconnect counter on successful connection
        self.login()

    def on_message(self, ws, message):
        try:
            data = json.loads(message)

            # Handle login response
            if "streamSessionId" in data:
                self.logged_in = True
                print("[INFO] Logged in successfully.")
                # Start heartbeat after successful login
                if not self.heartbeat_thread or not self.heartbeat_thread.is_alive():
                    self.start_heartbeat()

            # This is how XTB API returns command responses - it has both status and returnData
            elif "status" in data and "returnData" in data:
                # Store the most recent command response
                # Since XTB doesn't include the command name in the response, use the most recent command
                if hasattr(self, 'last_command') and self.last_command:
                    self.response_data[self.last_command] = data["returnData"]
                    print(f"[DEBUG] Stored response for command: {self.last_command}")
                    self.last_command = None  # Reset last command
                    self.command_lock = False  # Release lock
                else:
                    print(f"[DEBUG] Received response but can't match to command: {message[:100]}...")

            # Handle errors
            elif "errorDescr" in data:
                print(f"[ERROR] API error: {data.get('errorDescr', 'Unknown error')}")
                if hasattr(self, 'last_command') and self.last_command:
                    self.response_data[self.last_command] = {"error": data.get("errorDescr", "Unknown error")}
                    self.last_command = None
                    self.command_lock = False  # Release lock

            else:
                print(f"[DEBUG] Received unhandled message: {message[:100]}...")

        except Exception as e:
            print(f"[ERROR] Error processing message: {e}, Message: {message[:100]}")
            self.command_lock = False  # Release lock in case of error

    def on_error(self, ws, error):
        print(f"[ERROR] WebSocket error: {error}")
        print(f"[DEBUG] WebSocket state: logged_in={self.logged_in}")
        self.command_lock = False  # Release lock in case of error

    def on_close(self, ws, close_status_code=None, close_msg=None):
        print(f"[INFO] WebSocket connection closed. Status: {close_status_code}, Message: {close_msg}")
        self.logged_in = False
        self.command_lock = False  # Release lock

        # Attempt to reconnect if we're still running and haven't exceeded max reconnects
        if self.running and self.reconnect_count < self.max_reconnects:
            self.reconnect_count += 1
            backoff_time = min(30, 2 ** self.reconnect_count)  # Exponential backoff up to 30 seconds
            print(
                f"[INFO] Attempting to reconnect in {backoff_time} seconds... (Attempt {self.reconnect_count}/{self.max_reconnects})")
            time.sleep(backoff_time)
            self.connect()
        elif self.reconnect_count >= self.max_reconnects:
            print(f"[ERROR] Maximum reconnection attempts ({self.max_reconnects}) reached. Giving up.")

    def connect(self):
        """Establish WebSocket connection with better error handling"""
        try:
            if self.ws is not None:
                self.ws.close()

            self.ws = websocket.WebSocketApp(
                XTB_WS_URL,
                on_open=self.on_open,
                on_message=self.on_message,
                on_error=self.on_error,
                on_close=self.on_close
            )

            # Start WebSocket connection in a separate thread
            websocket_thread = Thread(target=self.ws.run_forever)
            websocket_thread.daemon = True  # Allow thread to exit when main program exits
            websocket_thread.start()

            # Wait for connection and login
            timeout = time.time() + 20  # 20s timeout (increased from 15s)
            while not self.logged_in and time.time() < timeout:
                time.sleep(0.5)

            if not self.logged_in:
                print("[WARNING] Connection established but login timed out")
                return False

            return True

        except Exception as e:
            print(f"[ERROR] Connection failed: {e}")
            return False

    def start_heartbeat(self):
        """Start heartbeat thread to keep connection alive"""

        def heartbeat_worker():
            print("[INFO] Starting heartbeat service")
            heartbeat_interval = 25  # seconds (reduced from 30)
            while self.running and self.logged_in:
                try:
                    # Use a lightweight command as heartbeat
                    status_cmd = {
                        "command": "ping",
                        "arguments": {}
                    }
                    if self.ws and self.ws.sock and self.ws.sock.connected:
                        self.ws.send(json.dumps(status_cmd))
                        print("[DEBUG] Heartbeat sent")
                    else:
                        print("[WARNING] Cannot send heartbeat, connection not active")
                        break
                except Exception as e:
                    print(f"[ERROR] Heartbeat error: {e}")
                    break

                # Sleep for the heartbeat interval
                time.sleep(heartbeat_interval)

            print("[INFO] Heartbeat service stopped")

        # Only start a new thread if one isn't already running
        if self.heartbeat_thread and self.heartbeat_thread.is_alive():
            print("[INFO] Heartbeat thread already running")
            return

        self.heartbeat_thread = Thread(target=heartbeat_worker)
        self.heartbeat_thread.daemon = True
        self.heartbeat_thread.start()

    def send_command(self, command, arguments=None):
        """Send command to XTB API with retry logic"""
        if not self.logged_in and command != "login":
            print("[ERROR] Not logged in yet.")
            return None

        # Check for command lock (simple concurrency control)
        timeout = time.time() + 10  # 10s timeout for lock (increased from 5s)
        while self.command_lock and time.time() < timeout:
            time.sleep(0.1)

        if self.command_lock:
            print(f"[ERROR] Command lock timeout for {command}")
            return None

        self.command_lock = True  # Acquire lock

        max_retries = 5  # Increased from 3
        for attempt in range(max_retries):
            try:
                payload = {"command": command}
                if arguments:
                    payload["arguments"] = arguments

                # Store command in response_data and track the last command
                self.response_data[command] = None
                self.last_command = command

                # Convert to JSON and send
                payload_str = json.dumps(payload)
                print(f"[DEBUG] Sending: {payload_str[:100]}")

                if not self.ws or not self.ws.sock or not self.ws.sock.connected:
                    print("[ERROR] WebSocket not connected")
                    self.connect()  # Try to reconnect
                    if not self.logged_in:
                        self.command_lock = False  # Release lock
                        return None

                self.ws.send(payload_str)

                # Wait for response with timeout
                timeout = time.time() + 45  # 45s timeout (increased from 30s)
                while self.response_data[command] is None and time.time() < timeout:
                    time.sleep(0.1)

                if self.response_data[command] is None:
                    print(
                        f"[WARNING] Timeout waiting for response to command: {command}, attempt {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:  # Only wait if we're going to retry
                        time.sleep(2 * (attempt + 1))  # Exponential backoff
                        continue
                    else:
                        self.command_lock = False  # Release lock
                else:
                    # Check for error in response
                    if isinstance(self.response_data[command], dict) and 'error' in self.response_data[command]:
                        print(f"[ERROR] API error for command {command}: {self.response_data[command]['error']}")
                        self.command_lock = False  # Release lock
                        return None

                    result = self.response_data.get(command)
                    self.command_lock = False  # Release lock
                    return result

            except Exception as e:
                print(f"[ERROR] Error sending command {command}: {e}")
                if attempt < max_retries - 1:  # Only wait if we're going to retry
                    time.sleep(2 * (attempt + 1))

        self.command_lock = False  # Release lock
        return None  # Return None if all retries failed

    def login(self):
        """Log in to XTB API"""
        login_cmd = {
            "command": "login",
            "arguments": {"userId": XTB_USER_ID, "password": XTB_PASSWORD}
        }
        print("[DEBUG] Sending login command")
        self.last_command = "login"  # Set this for the login command too

        try:
            self.ws.send(json.dumps(login_cmd))
        except Exception as e:
            print(f"[ERROR] Failed to send login command: {e}")

    def disconnect(self):
        """Cleanly disconnect from XTB"""
        self.running = False  # Stop reconnection attempts and heartbeat

        if self.logged_in:
            try:
                # Try to logout properly
                logout_cmd = {"command": "logout"}
                self.ws.send(json.dumps(logout_cmd))
                time.sleep(1)  # Give it a moment to process
            except:
                pass  # Ignore errors during logout

        if self.ws:
            try:
                self.ws.close()
            except:
                pass

        print("[INFO] Disconnected from XTB")


# Function to get all available stock symbols from XTB
def get_all_stock_symbols(client):
    print("[INFO] Retrieving all available stock symbols from XTB API")

    try:
        response = client.send_command("getAllSymbols", {})

        if response is None:
            print("[ERROR] Failed to fetch stock list.")
            return []

        # Filter to get only valid stock symbols
        stocks = []

        if isinstance(response, list):
            for item in response:
                if isinstance(item, dict) and "symbol" in item:
                    # Extract symbol and additional info
                    symbol = item.get("symbol", "")
                    category = item.get("categoryName", "")
                    description = item.get("description", "")

                    # Make sure it's a stock - filter by category if needed
                    # This filtering criteria might need adjustment based on XTB's categories
                    if symbol and len(symbol) > 0:
                        stocks.append({"symbol": symbol,
                                       "category": category,
                                       "description": description})

        print(f"[INFO] Found {len(stocks)} total symbols")
        return stocks
    except Exception as e:
        print(f"[ERROR] Error getting stock symbols: {e}")
        traceback.print_exc()
        return []


# Function to fetch historical stock data
def get_stock_data(client, symbol):
    print(f"[INFO] Fetching historical data for: {symbol}")
    try:
        # XTB uses UNIX timestamps in milliseconds (last 2 years for better analysis)
        end_time = int(time.time() * 1000)
        start_time = end_time - (2 * 365 * 24 * 60 * 60 * 1000)  # 2 years ago (increased from 1 year)
        arguments = {
            "info": {
                "symbol": symbol,
                "period": 1440,  # Daily (1440 minutes)
                "start": start_time,
                "end": end_time
            }
        }
        response = client.send_command("getChartLastRequest", arguments)

        if response is None:
            print(f"[WARNING] No response from API for {symbol}")
            return None

        if "rateInfos" not in response or not response["rateInfos"]:
            print(f"[WARNING] No historical data for {symbol}. Response: {response}")
            return None

        df = pd.DataFrame(response["rateInfos"])

        if df.empty:
            print(f"[WARNING] Empty dataframe for {symbol}")
            return None

        df["time"] = pd.to_datetime(df["ctm"], unit="ms")
        df["close"] = df["close"] + df["open"]  # XTB gives delta, we want absolute close
        df = df.set_index("time")

        # Add more price data columns
        if "open" in df.columns and "close" in df.columns:
            df["4. close"] = df["close"]
            df["high"] = df["open"] + df["high"]  # XTB gives deltas
            df["low"] = df["open"] + df["low"]  # XTB gives deltas
            df["volume"] = df["vol"]

            # Check for NaN values
            for col in ["open", "high", "low", "4. close", "volume"]:
                if df[col].isna().any():
                    print(f"[WARNING] NaN values found in {col}, filling forward")
                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

            print(f"[DEBUG] Processed data for {symbol}: {len(df)} records")
            return df[["open", "high", "low", "4. close", "volume"]]
        else:
            print(f"[WARNING] Missing required columns in {symbol} data")
            return None
    except Exception as e:
        print(f"[ERROR] Error processing data for {symbol}: {e}")
        traceback.print_exc()
        return None


# Enhanced technical indicators with mean reversion components
def calculate_technical_indicators(data):
    try:
        print(f"[DEBUG] Calculating enhanced technical indicators on data with shape: {data.shape}")
        df = data.copy()

        # Check if data is sufficient
        if len(df) < 50:
            print("[WARNING] Not enough data for technical indicators calculation")
            return None

        # Calculate returns
        df['returns'] = df['4. close'].pct_change()
        df['returns'] = df['returns'].fillna(0)

        # Calculate volatility (20-day rolling standard deviation)
        df['volatility'] = df['returns'].rolling(window=20).std()
        df['volatility'] = df['volatility'].fillna(0)

        # Calculate Simple Moving Averages
        df['SMA20'] = df['4. close'].rolling(window=20).mean()
        df['SMA50'] = df['4. close'].rolling(window=50).mean()
        df['SMA100'] = df['4. close'].rolling(window=100).mean()
        df['SMA200'] = df['4. close'].rolling(window=200).mean()

        # Fill NaN values in SMAs with forward fill then backward fill
        for col in ['SMA20', 'SMA50', 'SMA100', 'SMA200']:
            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

        # Calculate Relative Strength Index (RSI)
        delta = df['4. close'].diff()
        delta = delta.fillna(0)

        # Handle division by zero and NaN values in RSI calculation
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)

        avg_gain = gain.rolling(window=14).mean()
        avg_loss = loss.rolling(window=14).mean()

        # Handle zero avg_loss
        rs = np.zeros_like(avg_gain)
        valid_indices = avg_loss != 0
        rs[valid_indices] = avg_gain[valid_indices] / avg_loss[valid_indices]

        df['RSI'] = 100 - (100 / (1 + rs))
        df['RSI'] = df['RSI'].fillna(50)  # Default to neutral RSI (50)

        # Calculate Bollinger Bands
        df['BB_middle'] = df['SMA20']
        df['BB_std'] = df['4. close'].rolling(window=20).std()
        df['BB_std'] = df['BB_std'].fillna(0)
        df['BB_upper'] = df['BB_middle'] + (df['BB_std'] * 2)
        df['BB_lower'] = df['BB_middle'] - (df['BB_std'] * 2)

        # Calculate MACD
        df['EMA12'] = df['4. close'].ewm(span=12, adjust=False).mean()
        df['EMA26'] = df['4. close'].ewm(span=26, adjust=False).mean()
        df['MACD'] = df['EMA12'] - df['EMA26']
        df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
        df['MACD_hist'] = df['MACD'] - df['MACD_signal']

        # Calculate trading volume changes
        df['volume_change'] = df['volume'].pct_change()
        df['volume_change'] = df['volume_change'].fillna(0)

        # NEW INDICATORS - MEAN REVERSION COMPONENTS

        # 1. Distance from SMA200 as mean reversion indicator
        df['dist_from_SMA200'] = (df['4. close'] / df['SMA200']) - 1
        
        # 2. Bollinger Band %B (0-1 scale where >1 is overbought, <0 is oversold)
        bb_range = df['BB_upper'] - df['BB_lower']
        df['BB_pctB'] = np.where(
            bb_range > 0,
            (df['4. close'] - df['BB_lower']) / bb_range,
            0.5
        )
        
        # 3. Price Rate of Change (historical returns over different periods)
        df['ROC_5'] = df['4. close'].pct_change(5)
        df['ROC_10'] = df['4. close'].pct_change(10)
        df['ROC_20'] = df['4. close'].pct_change(20)
        df['ROC_60'] = df['4. close'].pct_change(60)
        
        # 4. Overbought/Oversold indicator based on historical returns
        # Standardize recent returns relative to their own history
        returns_z_score = lambda x: (x - x.rolling(60).mean()) / x.rolling(60).std()
        df['returns_zscore_5'] = returns_z_score(df['ROC_5'])
        df['returns_zscore_20'] = returns_z_score(df['ROC_20'])
        
        # 5. Price acceleration (change in ROC) - detects momentum exhaustion
        df['ROC_accel'] = df['ROC_5'] - df['ROC_5'].shift(5)
        
        # 6. Historical volatility ratio (recent vs long-term)
        df['vol_ratio'] = df['volatility'] / df['volatility'].rolling(60).mean()
        
        # 7. Mean reversion potential based on distance from long-term trend
        # Using Z-score of price deviation from 200-day SMA
        mean_dist = df['dist_from_SMA200'].rolling(100).mean()
        std_dist = df['dist_from_SMA200'].rolling(100).std()
        df['mean_reversion_z'] = np.where(
            std_dist > 0,
            (df['dist_from_SMA200'] - mean_dist) / std_dist,
            0
        )
        
        # 8. RSI divergence (price making new highs but RSI isn't)
        df['price_high'] = df['4. close'].rolling(10).max() == df['4. close']
        df['rsi_high'] = df['RSI'].rolling(10).max() == df['RSI']
        # Potential negative divergence: price high but RSI not high
        df['rsi_divergence'] = np.where(df['price_high'] & ~df['rsi_high'], -1, 0)
        
        # 9. Volume-price relationship (high returns with low volume can signal exhaustion)
        df['vol_price_ratio'] = np.where(
            df['returns'] != 0,
            df['volume'] / (abs(df['returns']) * df['4. close']),
            0
        )
        df['vol_price_ratio_z'] = (df['vol_price_ratio'] - df['vol_price_ratio'].rolling(20).mean()) / df['vol_price_ratio'].rolling(20).std()

        # 10. Stochastic Oscillator
        window = 14
        df['14-high'] = df['high'].rolling(window).max()
        df['14-low'] = df['low'].rolling(window).min()
        df['%K'] = (df['4. close'] - df['14-low']) * 100 / (df['14-high'] - df['14-low'])
        df['%D'] = df['%K'].rolling(3).mean()
        
        # 11. Advanced RSI Analysis
        # RSI slope (rate of change)
        df['RSI_slope'] = df['RSI'] - df['RSI'].shift(3)
        
        # RSI moving average crossovers
        df['RSI_MA5'] = df['RSI'].rolling(5).mean()
        df['RSI_MA14'] = df['RSI'].rolling(14).mean()
        
        # 12. Double Bollinger Bands (outer bands at 3 std dev)
        df['BB_upper_3'] = df['BB_middle'] + (df['BB_std'] * 3)
        df['BB_lower_3'] = df['BB_middle'] - (df['BB_std'] * 3)
        
        # 13. Volume Weighted MACD
        df['volume_ma'] = df['volume'].rolling(window=14).mean()
        volume_ratio = np.where(df['volume_ma'] > 0, df['volume'] / df['volume_ma'], 1)
        df['vol_weighted_macd'] = df['MACD'] * volume_ratio
        
        # 14. Chaikin Money Flow (CMF)
        money_flow_multiplier = ((df['4. close'] - df['low']) - (df['high'] - df['4. close'])) / (df['high'] - df['low'])
        money_flow_volume = money_flow_multiplier * df['volume']
        df['CMF'] = money_flow_volume.rolling(20).sum() / df['volume'].rolling(20).sum()
        
        # 15. Williams %R
        df['Williams_%R'] = -100 * (df['14-high'] - df['4. close']) / (df['14-high'] - df['14-low'])
        
        # 16. Advanced trend analysis
        df['trend_strength'] = np.abs(df['dist_from_SMA200'])
        df['price_vs_all_SMAs'] = np.where(
            (df['4. close'] > df['SMA20']) & 
            (df['4. close'] > df['SMA50']) & 
            (df['4. close'] > df['SMA100']) & 
            (df['4. close'] > df['SMA200']), 
            1, 0
        )
        
        # 17. SMA alignment (bullish/bearish alignment)
        df['sma_alignment'] = np.where(
            (df['SMA20'] > df['SMA50']) & 
            (df['SMA50'] > df['SMA100']) & 
            (df['SMA100'] > df['SMA200']), 
            1,  # Bullish alignment
            np.where(
                (df['SMA20'] < df['SMA50']) & 
                (df['SMA50'] < df['SMA100']) & 
                (df['SMA100'] < df['SMA200']), 
                -1,  # Bearish alignment
                0  # Mixed alignment
            )
        )
        
        # Fill NaN values in new indicators
        for col in df.columns:
            if df[col].isna().any():
                df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(0)

        print(f"[DEBUG] Enhanced technical indicators calculated successfully. New shape: {df.shape}")
        return df
    except Exception as e:
        print(f"[ERROR] Error calculating enhanced technical indicators: {e}")
        traceback.print_exc()
        return None


# PCA function to reduce dimensionality of features
def apply_pca(features_df):
    try:
        # Debug info about input
        print(f"[DEBUG] PCA input shape: {features_df.shape}")

        # Check if we have enough data
        if features_df.shape[0] < 10 or features_df.shape[1] < 5:
            print(f"[WARNING] Not enough data for PCA analysis: {features_df.shape}")
            return None, None

        # Select numerical columns that aren't NaN
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()

        # Exclude columns that are mostly NaN
        valid_cols = []
        for col in numeric_cols:
            if features_df[col].isna().sum() < len(features_df) * 0.3:  # At least 70% of values are not NaN (increased from 50%)
                valid_cols.append(col)

        if len(valid_cols) < 5:
            print(f"[WARNING] Not enough valid columns for PCA: {len(valid_cols)}")
            return None, None

        numeric_df = features_df[valid_cols].copy()

        # Fill remaining NaN values with column means
        for col in numeric_df.columns:
            if numeric_df[col].isna().any():
                numeric_df[col] = numeric_df[col].fillna(numeric_df[col].mean())

        print(f"[DEBUG] PCA numeric data shape after cleaning: {numeric_df.shape}")

        # Check for remaining NaN values
        if numeric_df.isna().sum().sum() > 0:
            print(f"[WARNING] NaN values still present after cleaning: {numeric_df.isna().sum().sum()}")
            # Replace remaining NaNs with 0
            numeric_df = numeric_df.fillna(0)

        # Standardize the features
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_df)

        # Apply PCA
        n_components = min(8, min(scaled_data.shape) - 1)  # Increased from 5
        pca = PCA(n_components=n_components)
        pca_results = pca.fit_transform(scaled_data)

        # Create a DataFrame with PCA results
        pca_df = pd.DataFrame(
            pca_results,
            columns=[f'PC{i + 1}' for i in range(pca_results.shape[1])],
            index=features_df.index
        )

        # Calculate explained variance for each component
        explained_variance = pca.explained_variance_ratio_

        print(f"[INFO] PCA explained variance: {explained_variance}")
        return pca_df, explained_variance
    except Exception as e:
        print(f"[ERROR] PCA failed: {e}")
        traceback.print_exc()
        return None, None


# Enhanced data preparation for LSTM prediction with additional features
def prepare_lstm_data(data, time_steps=60):  # Increased from 50
    try:
        # Check if we have enough data
        if len(data) < time_steps + 10:
            print(f"[WARNING] Not enough data for LSTM: {len(data)} < {time_steps + 10}")
            return None, None, None

        # Use multiple features instead of just closing prices
        # Include price, volume, and technical indicators if available
        features = []
        
        # Always include closing price
        features.append(data['4. close'].values)
        
        # Include volume if available with appropriate scaling
        if 'volume' in data.columns:
            # Log transform volume to reduce scale differences
            log_volume = np.log1p(data['volume'].values)
            features.append(log_volume)
        
        # Include volatility if available
        if 'volatility' in data.columns:
            features.append(data['volatility'].values)
            
        # Include RSI if available
        if 'RSI' in data.columns:
            # Normalize RSI to 0-1 scale
            normalized_rsi = data['RSI'].values / 100
            features.append(normalized_rsi)
            
        # Include MACD if available
        if 'MACD' in data.columns:
            # Normalize MACD using tanh for -1 to 1 range
            normalized_macd = np.tanh(data['MACD'].values / 5)
            features.append(normalized_macd)
            
        # Include Bollinger Band %B if available
        if 'BB_pctB' in data.columns:
            features.append(data['BB_pctB'].values)
            
        # Include distance from SMA200 (mean reversion indicator)
        if 'dist_from_SMA200' in data.columns:
            # Use tanh to normalize to -1 to 1 range
            normalized_dist = np.tanh(data['dist_from_SMA200'].values * 5)
            features.append(normalized_dist)
            
        # Include Williams %R if available
        if 'Williams_%R' in data.columns:
            # Normalize from -100-0 to 0-1
            normalized_williams = (data['Williams_%R'].values + 100) / 100
            features.append(normalized_williams)
            
        # Include CMF if available
        if 'CMF' in data.columns:
            # Already in -1 to 1 range
            features.append(data['CMF'].values)
            
        # Stack features
        feature_array = np.column_stack(features)
        
        # Check for NaN values across all features
        if np.isnan(feature_array).any():
            print(f"[WARNING] NaN values in features, filling with forward fill")
            # Convert to DataFrame for easier handling of NaNs
            temp_df = pd.DataFrame(feature_array)
            # Fill NaN values
            temp_df = temp_df.fillna(method='ffill').fillna(method='bfill')
            feature_array = temp_df.values

        # Normalize the data
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(feature_array)

        # Create sequences with all features
        X, y = [], []
        # Target is still the closing price (first feature)
        for i in range(len(scaled_features) - time_steps):
            X.append(scaled_features[i:i + time_steps])
            # For prediction target, use only the closing price column (index 0)
            y.append(scaled_features[i + time_steps, 0:1])

        # Convert to numpy arrays
        X = np.array(X)
        y = np.array(y)

        # Check shapes
        print(f"[DEBUG] Enhanced LSTM data shapes: X={X.shape}, y={y.shape}")

        return X, y, scaler
    except Exception as e:
        print(f"[ERROR] Error preparing enhanced LSTM data: {e}")
        traceback.print_exc()
        # Fallback to simpler preparation if enhanced fails
        try:
            print(f"[WARNING] Falling back to simple price-only LSTM preparation")
            # Get closing prices only
            prices = data['4. close'].values
            
            # Handle NaN values
            if np.isnan(prices).any():
                prices = pd.Series(prices).fillna(method='ffill').fillna(method='bfill').values
                
            # Reshape and scale
            prices_2d = prices.reshape(-1, 1)
            scaler = StandardScaler()
            scaled_prices = scaler.fit_transform(prices_2d)
            
            # Create sequences
            X, y = [], []
            for i in range(len(scaled_prices) - time_steps):
                X.append(scaled_prices[i:i + time_steps])
                y.append(scaled_prices[i + time_steps])
                
            # Convert to numpy arrays
            X = np.array(X)
            y = np.array(y)
            
            print(f"[DEBUG] Fallback LSTM data shapes: X={X.shape}, y={y.shape}")
            return X, y, scaler
            
        except Exception as e2:
            print(f"[ERROR] Fallback LSTM data preparation also failed: {e2}")
            return None, None, None


# Enhanced LSTM model for volatility prediction - maximized for M1 iMac
def build_lstm_model(input_shape):
    try:
        # Highly sophisticated architecture for maximum prediction accuracy
        inputs = Input(shape=input_shape)
        
        # First LSTM layer with more units
        x = LSTM(128, return_sequences=True)(inputs)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        # Second LSTM layer
        x = LSTM(128, return_sequences=True)(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        # Third LSTM layer
        x = LSTM(64, return_sequences=False)(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        # Dense layers for feature extraction
        x = Dense(64, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        # Final dense layer before output
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.1)(x)
        
        # Output layer
        outputs = Dense(1)(x)
        
        model = Model(inputs=inputs, outputs=outputs)
        
        # Use Adam optimizer with custom learning rate
        optimizer = Adam(learning_rate=0.001)
        model.compile(optimizer=optimizer, loss="mse")
        return model
    except Exception as e:
        print(f"[ERROR] Error building enhanced LSTM model: {e}")
        traceback.print_exc()
        
        # Fallback to simpler model if complex one fails
        try:
            inputs = Input(shape=input_shape)
            x = LSTM(64, return_sequences=True)(inputs)
            x = Dropout(0.2)(x)
            x = LSTM(64, return_sequences=False)(x)
            x = Dense(32, activation='relu')(x)
            outputs = Dense(1)(x)
            model = Model(inputs=inputs, outputs=outputs)
            model.compile(optimizer="adam", loss="mse")
            return model
        except Exception as e2:
            print(f"[ERROR] Fallback LSTM model also failed: {e2}")
            
            # Very simple fallback
            try:
                inputs = Input(shape=input_shape)
                x = LSTM(32, return_sequences=False)(inputs)
                outputs = Dense(1)(x)
                model = Model(inputs=inputs, outputs=outputs)
                model.compile(optimizer="adam", loss="mse")
                return model
            except Exception as e3:
                print(f"[ERROR] All LSTM model attempts failed: {e3}")
                return None


# Enhanced LSTM model training and prediction with extended processing time
def predict_with_lstm(data):
    try:
        # Set a maximum execution time - significantly increased for thorough training
        max_execution_time = 240  # 4 minutes max (increased from 2 minutes)
        start_time = time.time()

        # Require less data to attempt prediction
        if len(data) < 60:
            print("[WARNING] Not enough data for LSTM model")
            return 0

        # Use a larger window for more context
        time_steps = 60  # Increased for better prediction accuracy
        
        # Prepare data with enhanced features
        X, y, scaler = prepare_lstm_data(data, time_steps=time_steps)
        if X is None or y is None or scaler is None:
            print("[WARNING] Failed to prepare LSTM data")
            return 0

        # More lenient on required data size
        if len(X) < 8:
            print(f"[WARNING] Not enough data after preparation: {len(X)}")
            return 0

        # Build enhanced model
        model = build_lstm_model((X.shape[1], X.shape[2]))
        if model is None:
            print("[WARNING] Failed to build LSTM model")
            return 0

        # Use more training data for better learning
        max_samples = 1000  # Significantly increased from 500
        if len(X) > max_samples:
            # Use evenly spaced samples to get good representation
            indices = np.linspace(0, len(X) - 1, max_samples, dtype=int)
            X_train = X[indices]
            y_train = y[indices]
        else:
            X_train = X
            y_train = y

        # Use try/except for model training
        try:
            # Check if we're still within time limit
            if time.time() - start_time > max_execution_time:
                print("[WARNING] LSTM execution time limit reached before training")
                # Use a better fallback prediction based on recent volatility
                return data['volatility'].iloc[-15:].mean() / data['volatility'].iloc[-45:].mean()

            # Train model with more epochs and better callbacks
            early_stop = EarlyStopping(monitor='loss', patience=5, verbose=0)  # Increased patience
            reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.0001)

            # Set parameters for extensive training
            model.fit(
                X_train, y_train,
                epochs=30,  # Doubled from 15
                batch_size=32,
                callbacks=[early_stop, reduce_lr],
                verbose=0,
                shuffle=True
            )
            
            # Extra training round with lower learning rate for fine-tuning
            if time.time() - start_time < max_execution_time * 0.6:
                # Reduce learning rate for fine-tuning
                for layer in model.layers:
                    if hasattr(layer, 'optimizer'):
                        layer.optimizer.lr = layer.optimizer.lr * 0.3
                
                model.fit(
                    X_train, y_train, 
                    epochs=20,
                    batch_size=32,
                    verbose=0,
                    shuffle=True
                )
                
            # Final fine-tuning with small batch size if time permits
            if time.time() - start_time < max_execution_time * 0.8:
                model.fit(
                    X_train, y_train, 
                    epochs=10,
                    batch_size=16,  # Smaller batch size for final tuning
                    verbose=0,
                    shuffle=True
                )
                
        except Exception as e:
            print(f"[ERROR] LSTM model training failed: {e}")
            return 0

        # Make prediction for future volatility
        try:
            # Check time again
            if time.time() - start_time > max_execution_time:
                print("[WARNING] LSTM execution time limit reached before prediction")
                return 0.5  # Return a neutral value

            # Use ensemble of predictions from the last few sequences for better stability
            num_pred_samples = min(10, len(X))  # Increased from 5
            predictions = []
            
            for i in range(num_pred_samples):
                seq_idx = len(X) - i - 1
                if seq_idx >= 0:  # Check if index is valid
                    sequence = X[seq_idx].reshape(1, X.shape[1], X.shape[2])
                    pred = model.predict(sequence, verbose=0)[0][0]
                    predictions.append(pred)
            
            if not predictions:
                return 0.5  # Default if no valid predictions
                
            # Weight more recent predictions higher
            weights = np.linspace(1.0, 0.5, len(predictions))
            weights = weights / np.sum(weights)  # Normalize
            
            avg_prediction = np.sum(np.array(predictions) * weights)
            
            # Get weighted average of recent actual values
            last_actuals = y[-num_pred_samples:].flatten()
            last_actual_weights = np.linspace(1.0, 0.5, len(last_actuals))
            last_actual_weights = last_actual_weights / np.sum(last_actual_weights)
            last_actual = np.sum(last_actuals * last_actual_weights)

            # Avoid division by zero
            if abs(last_actual) < 1e-6:
                predicted_volatility_change = abs(avg_prediction)
            else:
                predicted_volatility_change = abs((avg_prediction - last_actual) / last_actual)

            print(f"[DEBUG] LSTM prediction: {predicted_volatility_change}")
            
            # Return a more nuanced measure capped at 1.0
            return min(1.0, max(0.1, predicted_volatility_change))
            
        except Exception as e:
            print(f"[ERROR] LSTM prediction failed: {e}")
            return 0
    except Exception as e:
        print(f"[ERROR] Error in LSTM prediction: {e}")
        traceback.print_exc()
        return 0


# Enhanced DQN Agent implementation with additional features for accuracy
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=10000)  # Substantially increased from 5000
        self.gamma = 0.98  # Increased from 0.97 for more future focus
        self.epsilon = 1.0
        self.epsilon_min = 0.03  # Lower min epsilon for better exploitation
        self.epsilon_decay = 0.97  # Slower decay for better exploration
        self.model = self._build_model()
        self.target_model = self._build_model()  # Separate target network
        self.target_update_counter = 0
        self.target_update_freq = 5  # Update target more frequently (was 10)
        self.max_training_time = 120  # 2 minutes maximum (doubled from 60s)
        self.batch_history = []  # Track training history

    def _build_model(self):
        try:
            # Advanced model architecture for superior learning
            model = Sequential([
                Dense(256, activation="relu", input_shape=(self.state_size,)),  # Double size
                BatchNormalization(),
                Dropout(0.3),  # More aggressive dropout
                Dense(256, activation="relu"),
                BatchNormalization(),
                Dropout(0.3),
                Dense(128, activation="relu"),
                Dropout(0.2),
                Dense(64, activation="relu"),
                Dropout(0.1),
                Dense(self.action_size, activation="linear")
            ])
            
            # Use Adam optimizer with custom learning rate
            optimizer = Adam(learning_rate=0.0005)
            model.compile(optimizer=optimizer, loss="mse")
            return model
        except Exception as e:
            print(f"[ERROR] Error building enhanced DQN model: {e}")
            
            # Fallback to simpler model
            try:
                model = Sequential([
                    Dense(128, activation="relu", input_shape=(self.state_size,)),
                    Dropout(0.2),
                    Dense(128, activation="relu"),
                    Dropout(0.2),
                    Dense(64, activation="relu"),
                    Dense(self.action_size, activation="linear")
                ])
                model.compile(optimizer="adam", loss="mse")
                return model
            except Exception as e2:
                print(f"[ERROR] Error building intermediate DQN model: {e2}")
                
                # Even simpler fallback model
                try:
                    model = Sequential([
                        Dense(64, activation="relu", input_shape=(self.state_size,)),
                        Dense(64, activation="relu"),
                        Dense(self.action_size, activation="linear")
                    ])
                    model.compile(optimizer="adam", loss="mse")
                    return model
                except Exception as e3:
                    print(f"[ERROR] Error building simplest DQN model: {e3}")
                    
                    # Final minimal fallback
                    try:
                        model = Sequential([
                            Dense(32, activation="relu", input_shape=(self.state_size,)),
                            Dense(self.action_size, activation="linear")
                        ])
                        model.compile(optimizer="adam", loss="mse")
                        return model
                    except Exception as e4:
                        print(f"[ERROR] All DQN model attempts failed: {e4}")
                        return None
    
    # Update target model (for more stable learning)
    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())
        print("[DEBUG] DQN target model updated")
        
    def remember(self, state, action, reward, next_state, done):
        # Only add to memory if not full
        if len(self.memory) < self.memory.maxlen:
            self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        try:
            if np.random.rand() <= self.epsilon:
                return random.randrange(self.action_size)
            if self.model is None:
                return random.randrange(self.action_size)
                
            # Get multiple predictions with noise for ensembling
            num_predictions = 3
            actions = []
            
            for _ in range(num_predictions):
                act_values = self.model.predict(state, verbose=0)
                # Add small noise for exploration
                act_values += np.random.normal(0, 0.05, size=act_values.shape)
                actions.append(np.argmax(act_values[0]))
                
            # Return most common action
            counts = np.bincount(actions)
            return np.argmax(counts)
            
        except Exception as e:
            print(f"[ERROR] Error in DQN act method: {e}")
            return random.randrange(self.action_size)

    def replay(self, batch_size):
        if len(self.memory) < batch_size or self.model is None:
            return

        # Add timeout mechanism
        start_time = time.time()

        try:
            # Track training iterations for adaptive learning
            train_iterations = 0
            
            # Use larger batch sizes for more stable learning
            actual_batch_size = min(batch_size, len(self.memory))
            minibatch = random.sample(self.memory, actual_batch_size)

            # Process in reasonable chunks for better performance
            chunk_size = 64  # Doubled from 32 for better batch learning
            
            for i in range(0, len(minibatch), chunk_size):
                chunk = minibatch[i:i + chunk_size]
                
                # Check timeout
                if time.time() - start_time > self.max_training_time:
                    print("[WARNING] DQN training timeout reached")
                    break

                # Process chunk
                states = np.vstack([x[0] for x in chunk])
                
                # Use the target network for more stable learning
                next_states = np.vstack([x[3] for x in chunk])
                actions = np.array([x[1] for x in chunk])
                rewards = np.array([x[2] for x in chunk])
                dones = np.array([x[4] for x in chunk])
                
                # Current Q values
                targets = self.model.predict(states, verbose=0)
                
                # Get next Q values from target model
                next_q_values = self.target_model.predict(next_states, verbose=0)
                
                # Update Q values - more efficient vectorized approach
                for j in range(len(chunk)):
                    if dones[j]:
                        targets[j, actions[j]] = rewards[j]
                    else:
                        # Add small noise to next state values for exploration
                        next_qs = next_q_values[j] + np.random.normal(0, 0.01, size=next_q_values[j].shape)
                        targets[j, actions[j]] = rewards[j] + self.gamma * np.max(next_qs)

                # Fit with more epochs for better learning
                history = self.model.fit(
                    states,
                    targets,
                    epochs=5,  # Increased from 3
                    batch_size=len(chunk),
                    verbose=0
                )
                
                # Track training progress
                self.batch_history.append(history.history['loss'][-1])
                train_iterations += 1

            # Update epsilon with a more gradual decay
            if self.epsilon > self.epsilon_min:
                # Adaptive decay based on memory size
                decay_rate = self.epsilon_decay + (0.01 * min(1.0, len(self.memory) / 5000))
                self.epsilon *= decay_rate
                self.epsilon = max(self.epsilon, self.epsilon_min)  # Ensure we don't go below min
                
            # Update target network periodically
            self.target_update_counter += 1
            if self.target_update_counter >= self.target_update_freq:
                self.update_target_model()
                self.target_update_counter = 0
                
            # Report training progress
            if self.batch_history:
                avg_loss = sum(self.batch_history[-train_iterations:]) / max(1, train_iterations)
                print(f"[DEBUG] DQN training - avg loss: {avg_loss:.5f}, epsilon: {self.epsilon:.3f}")

        except Exception as e:
            print(f"[ERROR] Error in DQN replay: {e}")
            traceback.print_exc()


# Enhanced DQN recommendation with more features and extensive training
def get_dqn_recommendation(data):
    try:
        # More lenient on required data
        if len(data) < 40:
            print("[WARNING] Not enough data for DQN")
            return 0.5  # Neutral score

        # Set timeout for the entire function - significantly increased for thorough training
        function_start_time = time.time()
        max_function_time = 240  # 4 minutes (doubled from 2 minutes)

        # Prepare state features with more historical context
        lookback = 15  # Further increased from 10 for better historical context
        
        # Extract more features for a richer state representation
        features = []
        
        # Basic indicators
        if 'returns' in data.columns:
            features.append(data['returns'].values[-lookback:])  # Returns
        if 'volatility' in data.columns:
            features.append(data['volatility'].values[-lookback:])  # Volatility
        
        # Technical indicators
        if 'RSI' in data.columns:
            rsi = data['RSI'].values[-lookback:] / 100  # Normalize to 0-1
            features.append(rsi)
        if 'MACD' in data.columns:
            macd = np.tanh(data['MACD'].values[-lookback:] / 5)
            features.append(macd)
        if 'SMA20' in data.columns and 'SMA50' in data.columns:
            sma20 = data['SMA20'].values[-lookback:]
            sma50 = data['SMA50'].values[-lookback:]
            with np.errstate(divide='ignore', invalid='ignore'):
                sma_ratio = np.where(sma50 != 0, sma20 / sma50, 1.0)
            sma_ratio = np.nan_to_num(sma_ratio, nan=1.0)
            sma_trend = np.tanh((sma_ratio - 1.0) * 5)
            features.append(sma_trend)
            
        # New mean reversion indicators
        if 'dist_from_SMA200' in data.columns:
            dist_sma200 = np.tanh(data['dist_from_SMA200'].values[-lookback:] * 5)
            features.append(dist_sma200)
        if 'BB_pctB' in data.columns:
            bb_pctb = data['BB_pctB'].values[-lookback:]
            # Transform to deviation from middle (0.5)
            bb_deviation = np.tanh((bb_pctb - 0.5) * 4)
            features.append(bb_deviation)
        if 'ROC_accel' in data.columns:
            roc_accel = np.tanh(data['ROC_accel'].values[-lookback:] * 10)
            features.append(roc_accel)
        if 'mean_reversion_z' in data.columns:
            mean_rev_z = np.tanh(data['mean_reversion_z'].values[-lookback:])
            features.append(mean_rev_z)
        if 'rsi_divergence' in data.columns:
            rsi_div = data['rsi_divergence'].values[-lookback:]
            features.append(rsi_div)
        if 'returns_zscore_20' in data.columns:
            ret_z = np.tanh(data['returns_zscore_20'].values[-lookback:])
            features.append(ret_z)
        if 'vol_ratio' in data.columns:
            vol_ratio = np.tanh((data['vol_ratio'].values[-lookback:] - 1) * 3)
            features.append(vol_ratio)
            
        # Additional indicators if available
        if 'Williams_%R' in data.columns:
            williams = (data['Williams_%R'].values[-lookback:] + 100) / 100
            features.append(williams)
        if 'CMF' in data.columns:
            cmf = data['CMF'].values[-lookback:]
            features.append(cmf)
        if 'sma_alignment' in data.columns:
            sma_align = data['sma_alignment'].values[-lookback:] / 2 + 0.5  # Convert -1,0,1 to 0,0.5,1
            features.append(sma_align)
        
        # Stack all features into the state
        features = [np.nan_to_num(f, nan=0.0) for f in features]  # Handle NaNs
        state = np.concatenate(features)

        # Define action space: 0=Sell, 1=Hold, 2=Buy
        action_size = 3
        agent = DQNAgent(state_size=len(state), action_size=action_size)

        if agent.model is None:
            print("[WARNING] Failed to create DQN model")
            return 0.5  # Neutral score

        # Use more training data for better learning
        max_train_points = min(500, len(data) - (lookback + 1))  # Increased from 200

        # Use appropriate step size to get good coverage of data
        step_size = max(1, (len(data) - (lookback + 1)) // 500)  # Adjusted for more points

        # First pass: collect experiences without training to populate memory
        print("[DEBUG] DQN collecting initial experiences...")
        
        # Track experience collection progress
        collection_start = time.time()
        experiences_collected = 0
        
        for i in range(0, max_train_points * step_size, step_size):
            # Check timeout
            if time.time() - function_start_time > max_function_time * 0.25:  # Use 25% of time for collection
                print(f"[WARNING] DQN experience collection timeout reached after {experiences_collected} experiences")
                break

            # Get index with bounds checking
            idx = min(i, len(data) - (lookback + 1))

            # Extract features for current state
            try:
                # Create state for this time point
                current_features = []
                
                # Collect same features for this specific time point
                for col_idx, col in enumerate(['returns', 'volatility', 'RSI', 'MACD']):
                    if col in data.columns and col_idx < len(features):
                        values = data[col].values[idx:idx + lookback]
                        if col == 'RSI':
                            values = values / 100
                        elif col == 'MACD':
                            values = np.tanh(values / 5)
                        current_features.append(np.nan_to_num(values, nan=0.0))
                
                # Add SMA trend if available
                if 'SMA20' in data.columns and 'SMA50' in data.columns and len(current_features) < len(features):
                    sma20_vals = data['SMA20'].values[idx:idx + lookback]
                    sma50_vals = data['SMA50'].values[idx:idx + lookback]
                    with np.errstate(divide='ignore', invalid='ignore'):
                        sma_ratio_vals = np.where(sma50_vals != 0, sma20_vals / sma50_vals, 1.0)
                    sma_ratio_vals = np.nan_to_num(sma_ratio_vals, nan=1.0)
                    sma_trend_vals = np.tanh((sma_ratio_vals - 1.0) * 5)
                    current_features.append(sma_trend_vals)
                
                # Add mean reversion features if available
                for col_idx, col in enumerate(['dist_from_SMA200', 'BB_pctB', 'ROC_accel', 'mean_reversion_z', 
                                              'rsi_divergence', 'returns_zscore_20', 'vol_ratio']):
                    if col in data.columns and len(current_features) < len(features):
                        values = data[col].values[idx:idx + lookback]
                        if col == 'dist_from_SMA200':
                            values = np.tanh(values * 5)
                        elif col == 'BB_pctB':
                            values = np.tanh((values - 0.5) * 4)
                        elif col in ['ROC_accel', 'mean_reversion_z', 'returns_zscore_20']:
                            values = np.tanh(values)
                        elif col == 'vol_ratio':
                            values = np.tanh((values - 1) * 3)
                        current_features.append(np.nan_to_num(values, nan=0.0))
                
                # Add additional indicators if available and used in feature set
                for col_idx, col in enumerate(['Williams_%R', 'CMF', 'sma_alignment']):
                    if col in data.columns and len(current_features) < len(features):
                        values = data[col].values[idx:idx + lookback]
                        if col == 'Williams_%R':
                            values = (values + 100) / 100
                        elif col == 'sma_alignment':
                            values = values / 2 + 0.5
                        current_features.append(np.nan_to_num(values, nan=0.0))
                
                # Fill missing features if necessary
                while len(current_features) < len(features):
                    current_features.append(np.zeros(lookback))
                
                # Create current state
                if len(current_features) > 0:
                    current_state = np.concatenate(current_features).reshape(1, len(state))
                else:
                    # Fallback if feature creation failed
                    current_state = np.zeros((1, len(state)))

                # Get next state (one step ahead)
                next_idx = idx + 1
                next_features = []
                
                if next_idx + lookback <= len(data):
                    # Repeat the feature collection for next state
                    # (For brevity, use a simplified collection for next state)
                    for col_idx, feat in enumerate(features):
                        if col_idx < len(features):
                            try:
                                # Slice the feature window for the next state
                                next_feat = features[col_idx][1:]  # Remove first element
                                # Append the next value if available
                                if next_idx + lookback - 1 < len(data):
                                    next_val = data.iloc[next_idx + lookback - 1].get(
                                        ['returns', 'volatility', 'RSI', 'MACD', 'SMA_trend', 
                                         'dist_from_SMA200', 'BB_pctB', 'ROC_accel'][col_idx % 8], 0)
                                else:
                                    next_val = 0
                                next_feat = np.append(next_feat, next_val)
                            except:
                                # If slicing fails, use zeros
                                next_feat = np.zeros_like(features[col_idx])
                                
                            next_features.append(next_feat)
                    
                    if len(next_features) > 0:
                        next_state = np.concatenate(next_features).reshape(1, len(state))
                    else:
                        next_state = np.zeros((1, len(state)))
                    
                    # Enhanced reward function with multiple factors
                    try:
                        # Base reward on forward return (1-day)
                        if next_idx + lookback < len(data):
                            price_return = data['returns'].values[next_idx + lookback - 1]
                        else:
                            price_return = 0
                            
                        # Add trend component based on multiple indicators
                        trend_component = 0
                        
                        # SMA trend contribution
                        if 'SMA20' in data.columns and 'SMA50' in data.columns and next_idx + lookback - 1 < len(data):
                            sma_trend_val = sma_trend[-1] if len(sma_trend) > 0 else 0
                            trend_component += sma_trend_val * 0.01
                            
                        # RSI contribution (favor mean-reversion for extreme values)
                        if 'RSI' in data.columns and next_idx + lookback - 1 < len(data):
                            rsi_val = data['RSI'].values[next_idx + lookback - 1]
                            # RSI mean reversion logic: high RSI > 70 is bearish, low RSI < 30 is bullish
                            if rsi_val > 70:
                                trend_component -= 0.01  # Bearish signal
                            elif rsi_val < 30:
                                trend_component += 0.01  # Bullish signal
                                
                        # MACD contribution
                        if 'MACD' in data.columns and 'MACD_hist' in data.columns and next_idx + lookback - 1 < len(data):
                            macd_val = data['MACD'].values[next_idx + lookback - 1]
                            macd_hist = data['MACD_hist'].values[next_idx + lookback - 1]
                            # MACD crossing above signal line is bullish
                            if macd_hist > 0 and macd_val > 0:
                                trend_component += 0.01
                            # MACD crossing below signal line is bearish
                            elif macd_hist < 0 and macd_val < 0:
                                trend_component -= 0.01
                        
                        # Combine components with directional awareness based on action
                        base_reward = price_return + trend_component
                        
                        # Get current action for this state
                        action = agent.act(current_state)
                        
                        # Adjust reward based on action-outcome alignment
                        # Buy (2) should be rewarded for positive returns, penalized for negative
                        # Sell (0) should be rewarded for negative returns, penalized for positive
                        # Hold (1) should be moderate in either case
                        if action == 2:  # Buy
                            reward = base_reward
                        elif action == 0:  # Sell
                            reward = -base_reward
                        else:  # Hold
                            reward = abs(base_reward) * 0.3  # Small positive reward for being right about direction
                        
                        # Add small penalty for extreme actions to encourage some holding
                        if action != 1:  # Not hold
                            reward -= 0.001  # Small transaction cost/risk penalty
                            
                        # Ensure reward is within reasonable bounds
                        reward = np.clip(reward, -0.1, 0.1)
                        
                        if np.isnan(reward):
                            reward = 0.0
                    except:
                        reward = 0.0

                    # Record experience
                    is_terminal = (next_idx + lookback >= len(data) - 1)
                    agent.remember(current_state, action, reward, next_state, is_terminal)
                    experiences_collected += 1
                
            except Exception as e:
                print(f"[WARNING] Error in DQN experience collection sample {i}: {e}")
                continue
                
        print(f"[INFO] Collected {experiences_collected} experiences in {time.time() - collection_start:.1f}s")

        # Second pass: train on collected experiences with progressive batch sizes
        if len(agent.memory) >= 64:
            print(f"[DEBUG] DQN training on {len(agent.memory)} experiences...")
            
            # Multiple training rounds with increasing batch sizes
            batch_sizes = [64, 128, 256, 512, 1024]
            for batch_size in batch_sizes:
                if batch_size <= len(agent.memory):
                    # Check if we still have time
                    if time.time() - function_start_time > max_function_time * 0.7:
                        print("[WARNING] DQN training timeout reached during main training")
                        break
                        
                    print(f"[DEBUG] DQN training with batch size {batch_size}")
                    agent.replay(batch_size)
                    
            # Final training with full memory if time permits
            if time.time() - function_start_time <= max_function_time * 0.9 and len(agent.memory) >= 1024:
                print("[DEBUG] DQN final training pass")
                agent.replay(min(2048, len(agent.memory)))

        # Get recommendation with ensemble predictions for robustness
        try:
            # Make multiple predictions with perturbations for robustness
            num_predictions = 10  # Increased from 5
            actions = []
            action_q_values = np.zeros(agent.action_size)
            
            for _ in range(num_predictions):
                # Apply small random noise to state
                perturbed_state = state.copy()
                perturbed_state += np.random.normal(0, 0.03, size=state.shape)  # 3% noise
                perturbed_state = perturbed_state.reshape(1, len(state))
                
                # Get Q values
                act_values = agent.model.predict(perturbed_state, verbose=0)[0]
                
                # Record action and accumulate Q values
                action = np.argmax(act_values)
                actions.append(action)
                action_q_values += act_values
                
            # Calculate average Q values
            action_q_values /= num_predictions
            
            # Determine ensemble actions and confidence
            most_common_action = max(set(actions), key=actions.count)
            action_confidence = actions.count(most_common_action) / num_predictions
            
            # Get Q value spread (difference between best and second best)
            sorted_q = np.sort(action_q_values)
            q_spread = (sorted_q[-1] - sorted_q[-2]) / (np.max(action_q_values) - np.min(action_q_values) + 1e-6)
            
            print(f"[DEBUG] DQN ensemble - action: {most_common_action}, confidence: {action_confidence:.2f}, Q spread: {q_spread:.2f}")
            
            # Convert action to score: 0=0.0, 1=0.5, 2=1.0 but adjust based on confidence
            if most_common_action == 1:  # Hold
                # For hold, start at 0.5 but adjust slightly based on Q values
                q_diff = action_q_values[2] - action_q_values[0]  # Buy - Sell
                # Scale to small adjustment around 0.5
                adjustment = np.tanh(q_diff) * 0.1
                dqn_score = 0.5 + adjustment
            else:
                # For buy/sell, use basic mapping but adjust by confidence
                base_score = most_common_action / 2  # 0->0.0, 2->1.0
                # Move score closer to neutral (0.5) if confidence is low
                confidence_factor = action_confidence * q_spread  # Combine both confidence measures
                dqn_score = 0.5 + (base_score - 0.5) * (0.5 + confidence_factor/2)
            
            # Ensure score is in 0-1 range
            dqn_score = max(0, min(1, dqn_score))
            
            return dqn_score
            
        except Exception as e:
            print(f"[WARNING] Error getting DQN action: {e}")
            return 0.5  # Neutral score

    except Exception as e:
        print(f"[ERROR] Error in DQN recommendation: {e}")
        traceback.print_exc()
        return 0.5  # Neutral score


# Enhanced Sigma metric calculation with mean reversion balance
def calculate_sigma(data):
    try:
        # Set a maximum execution time for the entire function
        max_execution_time = 600  # 10 minutes max (doubled from 5)
        start_time = time.time()

        # 1. Calculate technical indicators with mean reversion components
        indicators_df = calculate_technical_indicators(data)
        if indicators_df is None or len(indicators_df) < 30:
            print("[WARNING] Technical indicators calculation failed or insufficient data")
            return None

        # Intermediate check time after technical indicators calculation
        # More lenient threshold to allow continuing to advanced analysis
        if time.time() - start_time > max_execution_time * 0.2:
            print("[WARNING] Technical indicators calculation took longer than expected")
            print("[INFO] Continuing with all analysis methods, but will monitor time closely")

        # 2. Apply PCA to reduce feature dimensionality
        pca_results = None
        pca_variance = []
        pca_components = None

        # Only skip PCA if very constrained on time
        if time.time() - start_time < max_execution_time * 0.6:  # More generous allocation
            try:
                # Use more historical data for PCA
                lookback_period = min(120, len(indicators_df))  # Doubled from 60
                pca_results, pca_variance = apply_pca(indicators_df.iloc[-lookback_period:])
                
                if pca_results is not None:
                    # Store pca components for possible use in final sigma calculation
                    pca_components = pca_results.iloc[-1].values
                    print(f"[DEBUG] PCA components for latest datapoint: {pca_components}")
            except Exception as e:
                print(f"[WARNING] PCA calculation failed: {e}, continuing without it")
                pca_variance = []
        else:
            print("[WARNING] Skipping PCA calculation due to significant time constraints")

        # 3. Get LSTM volatility prediction
        lstm_prediction = 0
        if time.time() - start_time < max_execution_time * 0.7:
            lstm_prediction = predict_with_lstm(data)
            print(f"[DEBUG] LSTM prediction: {lstm_prediction}")
        else:
            print("[WARNING] Skipping LSTM prediction due to time constraints")

        # 4. Get DQN recommendation
        dqn_recommendation = 0.5  # Default neutral
        if time.time() - start_time < max_execution_time * 0.8:
            dqn_recommendation = get_dqn_recommendation(indicators_df)
            print(f"[DEBUG] DQN recommendation: {dqn_recommendation}")
        else:
            print("[WARNING] Skipping DQN recommendation due to time constraints")

        # Get latest technical indicators
        latest = indicators_df.iloc[-1]
        
        # MOMENTUM INDICATORS
        traditional_volatility = indicators_df['volatility'].iloc[-1] if not np.isnan(indicators_df['volatility'].iloc[-1]) else 0
        rsi = latest['RSI'] if not np.isnan(latest['RSI']) else 50
        rsi_signal = (max(0, min(100, rsi)) - 30) / 70
        rsi_signal = max(0, min(1, rsi_signal))
        macd = latest['MACD'] if not np.isnan(latest['MACD']) else 0
        macd_signal = np.tanh(macd * 10)
        sma20 = latest['SMA20'] if not np.isnan(latest['SMA20']) else 1
        sma50 = latest['SMA50'] if not np.isnan(latest['SMA50']) else 1
        sma_trend = (sma20 / sma50 - 1) if abs(sma50) > 1e-6 else 0
        sma_signal = np.tanh(sma_trend * 10)
        
        # Calculate short-term momentum (last 10 days vs previous 10 days)
        try:
            recent_returns = indicators_df['returns'].iloc[-10:].mean()
            previous_returns = indicators_df['returns'].iloc[-20:-10].mean()
            momentum_signal = np.tanh((recent_returns - previous_returns) * 20)  # Scale to approx -1 to 1
            momentum_signal = (momentum_signal + 1) / 2  # Convert to 0-1 scale
        except:
            momentum_signal = 0.5  # Neutral
        
        # MEAN REVERSION INDICATORS
        
        # 1. Overbought/Oversold based on distance from SMA200
        dist_from_sma200 = latest['dist_from_SMA200'] if not np.isnan(latest['dist_from_SMA200']) else 0
        # Transform to a 0-1 signal where closer to 0 is more overbought (market reversal potential)
        sma200_signal = 1 - min(1, max(0, (dist_from_sma200 + 0.1) / 0.2))
        
        # 2. Bollinger Band %B signal (reverses when closer to extremes)
        bb_pctb = latest['BB_pctB'] if not np.isnan(latest['BB_pctB']) else 0.5
        # Transform so that extreme values (near 0 or 1) give higher reversal signals
        bb_reversal_signal = 1 - 2 * abs(bb_pctb - 0.5)
        bb_reversal_signal = max(0, min(1, bb_reversal_signal + 0.5))  # Rescale to 0-1
        
        # 3. Recent price acceleration (negative means momentum slowing)
        roc_accel = latest['ROC_accel'] if not np.isnan(latest['ROC_accel']) else 0
        # Transform to 0-1 signal where negative acceleration gives higher reversal signal
        accel_signal = max(0, min(1, 0.5 - roc_accel * 10))
        
        # 4. Mean reversion Z-score (extreme values suggest reversal potential)
        mean_rev_z = latest['mean_reversion_z'] if not np.isnan(latest['mean_reversion_z']) else 0
        # Transform to 0-1 signal where larger absolute z-score suggests higher reversal potential
        mean_rev_signal = min(1, abs(mean_rev_z) / 2)
        
        # 5. RSI divergence signal
        rsi_div = latest['rsi_divergence'] if not np.isnan(latest['rsi_divergence']) else 0
        # Transform to a 0-1 signal (1 = strong divergence)
        rsi_div_signal = 1 if rsi_div < 0 else 0
        
        # 6. Overbought detection based on returns Z-score
        returns_z = latest['returns_zscore_20'] if not np.isnan(latest['returns_zscore_20']) else 0
        # High positive z-score suggests overbought conditions
        overbought_signal = max(0, min(1, (returns_z + 1) / 4))
        
        # 7. Recent volatility increase (higher vol can precede reversals)
        vol_ratio = latest['vol_ratio'] if not np.isnan(latest['vol_ratio']) else 1
        vol_increase_signal = max(0, min(1, (vol_ratio - 0.8) / 1.2))
        
        # 8. Additional indicators if available
        williams_r = (latest['Williams_%R'] + 100) / 100 if 'Williams_%R' in latest and not np.isnan(latest['Williams_%R']) else 0.5
        cmf = (latest['CMF'] + 1) / 2 if 'CMF' in latest and not np.isnan(latest['CMF']) else 0.5
        
        # Component groups for Sigma calculation
        momentum_components = {
            "rsi": rsi_signal,
            "macd": (macd_signal + 1) / 2,  # Convert from -1:1 to 0:1
            "sma_trend": (sma_signal + 1) / 2,  # Convert from -1:1 to 0:1
            "traditional_volatility": min(1, traditional_volatility * 25),
            "momentum": momentum_signal,
            "williams_r": williams_r,
            "cmf": cmf,
            "lstm": lstm_prediction,
            "dqn": dqn_recommendation
        }
        
        # Mean reversion components (higher value = higher reversal potential)
        reversion_components = {
            "sma200_signal": sma200_signal,  
            "bb_reversal": bb_reversal_signal,
            "accel_signal": accel_signal,
            "mean_rev_signal": mean_rev_signal,
            "rsi_div_signal": rsi_div_signal,
            "overbought_signal": overbought_signal,
            "vol_increase_signal": vol_increase_signal
        }
        
        print(f"[DEBUG] Momentum components: {momentum_components}")
        print(f"[DEBUG] Mean reversion components: {reversion_components}")
        
        # Calculate momentum score (bullish when high)
        if lstm_prediction > 0 and dqn_recommendation != 0.5:
            # Full momentum score with all advanced components
            momentum_score = (
                0.15 * momentum_components["traditional_volatility"] +
                0.10 * momentum_components["rsi"] +
                0.10 * momentum_components["macd"] +
                0.10 * momentum_components["sma_trend"] +
                0.10 * momentum_components["momentum"] +
                0.05 * momentum_components["williams_r"] +
                0.05 * momentum_components["cmf"] +
                0.15 * momentum_components["lstm"] +
                0.20 * momentum_components["dqn"]
            )
        else:
            # Simplified momentum score without advanced models
            momentum_score = (
                0.20 * momentum_components["traditional_volatility"] +
                0.15 * momentum_components["rsi"] +
                0.15 * momentum_components["macd"] +
                0.15 * momentum_components["sma_trend"] +
                0.15 * momentum_components["momentum"] +
                0.10 * momentum_components["williams_r"] +
                0.10 * momentum_components["cmf"]
            )
        
        # Calculate mean reversion score (bearish when high)
        reversion_score = (
            0.20 * reversion_components["sma200_signal"] +
            0.15 * reversion_components["bb_reversal"] +
            0.15 * reversion_components["accel_signal"] +
            0.15 * reversion_components["mean_rev_signal"] +
            0.10 * reversion_components["rsi_div_signal"] +
            0.15 * reversion_components["overbought_signal"] +
            0.10 * reversion_components["vol_increase_signal"]
        )
        
        # Get recent monthly return
        recent_returns = latest['ROC_20'] if 'ROC_20' in latest and not np.isnan(latest['ROC_20']) else 0
        
        # Combine scores with balance factor to get final Sigma
        # Higher balance_factor means more weight to mean reversion
        base_balance_factor = 0.5  # Equal weight to momentum and mean reversion
        
        # For stocks with recent large moves, increase the mean reversion weight
        if recent_returns > 0.15:  # >15% monthly returns
            # Gradually increase mean reversion weight for higher recent returns
            excess_return_factor = min(0.3, (recent_returns - 0.15) * 2)  # Up to 0.3 extra weight
            balance_factor = base_balance_factor + excess_return_factor
            print(f"[INFO] Increasing mean reversion weight by {excess_return_factor:.2f} due to high recent returns ({recent_returns:.1%})")
        elif recent_returns < -0.15:  # <-15% monthly returns (big drop)
            # For big drops, slightly reduce mean reversion weight (they've already reverted)
            balance_factor = max(0.3, base_balance_factor - 0.1)
            print(f"[INFO] Decreasing mean reversion weight due to significant recent decline ({recent_returns:.1%})")
        else:
            balance_factor = base_balance_factor
        
        # Calculate final sigma with balanced approach
        sigma = momentum_score * (1 - balance_factor) + (1 - reversion_score) * balance_factor
        
        # Add small PCA adjustment if available
        if pca_components is not None and len(pca_components) >= 3:
            # Use first few principal components to slightly adjust sigma
            pca_influence = np.tanh(np.sum(pca_components[:3]) / 3) * 0.05
            sigma += pca_influence
            print(f"[DEBUG] PCA adjustment to Sigma: {pca_influence:.3f}")
        
        # Ensure sigma is between 0 and 1
        sigma = max(0, min(1, sigma))
        
        print(f"[INFO] Final components: Momentum={momentum_score:.3f}, Reversion={reversion_score:.3f}, Balance={balance_factor:.2f}, Sigma={sigma:.3f}")
        
        # Analysis details
        analysis_details = {
            "sigma": sigma,
            "momentum_score": momentum_score,
            "reversion_score": reversion_score,
            "balance_factor": balance_factor,
            "recent_monthly_return": recent_returns,
            "traditional_volatility": traditional_volatility,
            "rsi": rsi,
            "macd": macd,
            "sma_trend": sma_trend,
            "dist_from_sma200": dist_from_sma200,
            "last_price": latest['4. close'] if not np.isnan(latest['4. close']) else 0,
            "lstm_prediction": lstm_prediction,
            "dqn_recommendation": dqn_recommendation,
        }
        
        return analysis_details
    except Exception as e:
        print(f"[ERROR] Error calculating balanced Sigma: {e}")
        traceback.print_exc()
        return None


# Enhanced recommendation function with mean reversion context
def get_sigma_recommendation(sigma, analysis_details):
    # Get additional context for our recommendation
    momentum_score = analysis_details.get("momentum_score", 0.5) 
    reversion_score = analysis_details.get("reversion_score", 0.5)
    recent_monthly_return = analysis_details.get("recent_monthly_return", 0)
    balance_factor = analysis_details.get("balance_factor", 0.5)
    
    # Base recommendation on sigma
    if sigma > 0.8:
        base_rec = "STRONG BUY"
    elif sigma > 0.6:
        base_rec = "BUY"
    elif sigma > 0.4:
        base_rec = "HOLD"
    elif sigma > 0.2:
        base_rec = "SELL"
    else:
        base_rec = "STRONG SELL"
    
    # Add nuanced context based on recent performance
    if recent_monthly_return > 0.25 and sigma > 0.6:
        context = f"Strong momentum with +{recent_monthly_return:.1%} monthly gain, but elevated mean reversion risk"
    elif recent_monthly_return > 0.15 and sigma > 0.6:
        context = f"Good momentum with +{recent_monthly_return:.1%} monthly gain, continue monitoring for exhaustion"
    elif recent_monthly_return > 0.10 and sigma > 0.6:
        context = f"Sustainable momentum with +{recent_monthly_return:.1%} monthly gain and minimal reversal signals"
    elif recent_monthly_return < -0.20 and sigma > 0.6:
        context = f"Strong reversal potential after {recent_monthly_return:.1%} monthly decline, indicators show bottoming"
    elif recent_monthly_return < -0.15 and sigma < 0.4:
        context = f"Continued weakness with {recent_monthly_return:.1%} monthly loss and limited reversal signals"
    elif recent_monthly_return < -0.10 and sigma > 0.5:
        context = f"Potential stabilization after {recent_monthly_return:.1%} monthly decline, monitor for trend change"
    else:
        # Default context with momentum/reversion balance
        if momentum_score > 0.7 and reversion_score < 0.3:
            context = "Strong trend with minimal reversal signals"
        elif momentum_score > 0.7 and reversion_score > 0.5:
            context = "Strong but potentially overextended momentum"
        elif momentum_score < 0.3 and reversion_score < 0.3:
            context = "Weak momentum with limited reversal potential"
        elif momentum_score < 0.3 and reversion_score > 0.5:
            context = "Weak momentum with strong bearish signals"
        elif momentum_score > 0.5 and reversion_score > 0.5:
            context = "Mixed signals - strong momentum facing reversal pressures"
        elif abs(momentum_score - (1-reversion_score)) < 0.1:
            context = "Balanced indicators with no clear directional edge"
        else:
            context = "Mixed signals requiring close monitoring for trend clarity"
    
    # Add balance factor context if extreme
    if balance_factor > 0.7:
        context += f" (Mean reversion factors heavily weighted due to technical extremes)"
    elif balance_factor < 0.3:
        context += f" (Momentum factors weighted more heavily due to market conditions)"
        
    # Combine base recommendation with context
    recommendation = f"{base_rec} - {context}"
    
    return recommendation


# Create or initialize the output file 
def initialize_output_file():
    try:
        with open(OUTPUT_FILE, "w") as file:
            file.write("===== XTB STOCK ANALYSIS DATABASE =====\n")
            file.write(f"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            file.write("FORMAT: TICKER | PRICE | SIGMA | RECOMMENDATION\n")
            file.write("----------------------------------------\n")
        return True
    except Exception as e:
        print(f"[ERROR] Failed to initialize output file: {e}")
        return False


# Append stock analysis result to the output file
def append_stock_result(symbol, price, sigma, recommendation):
    try:
        with open(OUTPUT_FILE, "a") as file:
            # Format: TICKER | PRICE | SIGMA | RECOMMENDATION
            file.write(f"{symbol} | ${price:.2f} | {sigma:.5f} | {recommendation}\n")
        return True
    except Exception as e:
        print(f"[ERROR] Failed to append result for {symbol}: {e}")
        return False


# Function to analyze a single stock with timeout
def analyze_stock(client, symbol, max_time=MAX_EXECUTION_TIME_PER_STOCK):
    print(f"[INFO] Analyzing stock: {symbol}")
    start_time = time.time()
    
    try:
        # Get historical data
        data = get_stock_data(client, symbol)
        
        # Check if we got valid data
        if data is None or len(data) < 60:
            print(f"[WARNING] Insufficient data for {symbol}")
            return None
            
        # Check if we're still within time limit
        if time.time() - start_time > max_time * 0.4:  # Increased from 0.5
            print(f"[WARNING] Data retrieval for {symbol} took too long")
            return None
            
        # Calculate Sigma
        analysis = calculate_sigma(data)
        
        if analysis is None:
            print(f"[WARNING] Failed to calculate Sigma for {symbol}")
            return None
            
        # Get enhanced recommendation
        sigma = analysis["sigma"]
        recommendation = get_sigma_recommendation(sigma, analysis)
        price = analysis["last_price"]
        
        print(f"[INFO] Analysis complete for {symbol}: Sigma={sigma:.5f}, Recommendation={recommendation}")
        
        # Return the result
        return {
            "symbol": symbol,
            "price": price,
            "sigma": sigma,
            "recommendation": recommendation
        }
        
    except Exception as e:
        print(f"[ERROR] Error analyzing {symbol}: {e}")
        traceback.print_exc()
        return None
    finally:
        elapsed_time = time.time() - start_time
        print(f"[INFO]